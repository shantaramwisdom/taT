sqlFile="/application/financedw/scripts/Query"
logFilePath="/application/financedw/curated/Logs"
tempdir="/application/financedw/curated/tmp"
cluster_id=$(cat /mnt/var/lib/info/job-flow.json | jq -r '.jobFlowId')
env=$(aws emr describe-cluster --cluster-id $cluster_id --query Cluster.Tags | jq -r '.[] | select(.Key=="Environment") | jq -r '.Value')
cluster_name=$(aws emr describe-cluster --cluster-id $cluster_id --query Cluster.Tags | jq -r '.[] | select(.Key=="Name") | jq -r '.Value')
project=$(echo ${cluster_name} | cut -d'-' -f9)
emr_name_short=`echo ${cluster_name} | cut -d'-' -f8`
emr_name_short_check=`echo ${cluster_name} | cut -d'-' -f7`

if [ $project != 'financedw' ]; then
 project_ovrd=$project
else
 project_ovrd=''
fi

if [ ${emr_name_short_check} != 'all' ]; then
 emr_name_short=${emr_name_short_check}
fi

if [ ${env} == 'dev' ] || [ ${env} == 'tst' ]; then
 spark_properties="--deploy-mode client --executor-memory 30G --conf spark.dynamicAllocation.enabled=true --conf spark.dynamicAllocation.maxExecutors=20 \
 --executor-cores 5 --driver-memory 18G --conf spark.port.maxRetries=50 --conf spark.sql.broadcastTimeout=2000"
else
 spark_properties="--deploy-mode client --executor-memory 45G --conf spark.dynamicAllocation.enabled=true --conf spark.dynamicAllocation.maxExecutors=30 \
 --executor-cores 5 --driver-memory 30G --conf spark.port.maxRetries=50 --conf spark.sql.broadcastTimeout=2000"
fi

#if [ ${env} == 'dev' ] || [ ${env} == 'tst' ]; then
# spark_properties="--deploy-mode client --master yarn --driver-cores 5 --executor-cores 8 --driver-memory 90G --executor-memory 32G \
# --conf spark.dynamicAllocation.enabled=true --conf spark.dynamicAllocation.maxExecutors=15 --conf spark.sql.broadcastTimeout=2000 --conf spark.sql.autoBroadcastJoinThreshold=50M --conf spark.default.parallelism=2000 \
# --conf spark.shuffle.service.enabled=true --conf spark.shuffle.partitions=2000 \
# --conf spark.yarn.maxAppAttempts=1 --conf spark.driver.memoryOverhead=8G --conf spark.driver.extraJavaOptions=-Xss4M \
# --conf spark.shuffle.spill.numElementsForceSpillThreshold=500000 --conf spark.kryoserializer.buffer.max=2000m \
# --conf spark.sql.parquet.fs.optimized.committer.optimization-enabled=true --conf spark.sql.files.maxRecordsPerFile=0"
#else
# spark_properties="--deploy-mode client --master yarn --driver-cores 5 --executor-cores 8 --driver-memory 90G --executor-memory 32G \
# --conf spark.dynamicAllocation.enabled=true --conf spark.dynamicAllocation.maxExecutors=25 --conf spark.sql.broadcastTimeout=2000 --conf spark.sql.autoBroadcastJoinThreshold=50M --conf spark.default.parallelism=2000 \
# --conf spark.shuffle.service.enabled=true --conf spark.shuffle.partitions=2000 \
# --conf spark.yarn.maxAppAttempts=1 --conf spark.driver.memoryOverhead=8G --conf spark.driver.extraJavaOptions=-Xss4M \
# --conf spark.shuffle.spill.numElementsForceSpillThreshold=500000 --conf spark.kryoserializer.buffer.max=2000m \
# --conf spark.sql.parquet.fs.optimized.committer.optimization-enabled=true --conf spark.sql.files.maxRecordsPerFile=0"
#fi
